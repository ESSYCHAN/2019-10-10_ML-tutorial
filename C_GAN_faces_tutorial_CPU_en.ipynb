{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "debut = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial aims to see how a generative opposition network (GAN) can generate new faces after seeing many photos of real faces.\n",
    "\n",
    "It is very, very strongly inspired by the pytorch tutorial : [dcgan_faces_tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a GAN?\n",
    "\n",
    "GANs are a teaching model of a DL model to capture the distribution of training data in order to generate new data from the same distribution. GANs were invented by Ian Goodfellow in 2014 and first described in [Generative Adversarial Nets](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf). They are made up of two distinct models, a **generator** and a **discriminator**:\n",
    "\n",
    "- The generator produces false images that look like training images.\n",
    "- The discriminator looks at an image and displays whether or not it is a real training image or a false image from the generator.\n",
    "\n",
    "During training, the generator constantly tries to outperform the discriminator by generating better fakes, while the discriminator strives to become a better detective and correctly classify real and false images. The balance of this game is when the generator generates perfect fakes that seem to come directly from the training data, and the discriminator must always guess at 50% that the generator output is real or false.\n",
    "\n",
    "<img src=\"https://www.kdnuggets.com/wp-content/uploads/generative-adversarial-network.png\" width=\"900\"/>\n",
    "\n",
    "\n",
    "Some notes :\n",
    "\n",
    "- For the discriminator:\n",
    "    - **$x$** : data representing an image.\n",
    "    - **$D(x)$ : the network of discriminators** that provides the (scalar) probability that $x$ comes from the training data rather than the generator. The entry at $D(x)$ is an RGB image of size 3x64x64. $D(x)$ should be HIGH when $x$ comes from real data and LOW when $x$ comes from the generator. $D(x)$ can also be considered as a binary classifier.\n",
    "\n",
    "- For the generator:\n",
    "    - **$z$** : a latent space vector sampled from a standard normal distribution.\n",
    "    - **$G(z)$ : the generator function** that maps the latent vector $z$ to the data space. The objective of $G$ is to estimate the distribution from which the training data originates ($p_{data}$) in order to generate false samples from this estimated distribution ($p_g$).\n",
    "\n",
    "**$D(G(z))$ is the (scalar) probability that the output of the $G$ generator is a real image**.\n",
    "\n",
    "As described in [Goodfellow's article](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf), $D$ and $G$ play a minimax game :\n",
    "- **$D$** tries to **maximize** the probability that it correctly classifies the real and false ($logD(x)$)\n",
    "- **$G$** tries to **minimize** the probability that $D$ will predict that its results will be false ($log(1-D(G(x))$)). \n",
    "\n",
    "From the paper, the GAN loss function is as follows:\n",
    "\n",
    "\\begin{align}\\underset{G}{\\text{min}}~\\underset{D}{\\text{max}}~V(D,G) = \\mathbb{E}_{x\\sim p_{data}(x)}\\big[logD(x)\\big] + \\mathbb{E}_{z\\sim p_{z}(z)}\\big[log(1-D(G(z)))\\big]\\end{align}\n",
    "\n",
    "In theory, **the solution to this minimax set is where $p_g = p_{data}$**, and the discriminator randomly guesses whether the entries are real or false.\n",
    "\n",
    "However, GAN convergence theory is still under active research and, in reality, models do not always practice to this extent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a DCGAN?\n",
    "\n",
    "A DCGAN is a direct extension of GAN, except that it explicitly uses convolutional and convolutional-transpositive layers in the discriminator and generator, respectively. It was first described by Radford et al. in [Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks](https://arxiv.org/pdf/1511.06434.pdf).\n",
    "\n",
    "- The discriminator is composed of strided [convolution](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d) layers, [BatchNorm](https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d) layers and activation layers [LeakyReLU](https://pytorch.org/docs/stable/nn.html#torch.nn.LeakyReLU). The input is a 3x64x64 input image and the output is a scalar probability that the input comes from the actual distribution of the data.\n",
    "\n",
    "- The generator is composed of [convolutional-transpose](https://pytorch.org/docs/stable/nn.html#torch.nn.ConvTranspose2d) layers, [BatchNorm](https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d) layers and [ReLU](https://pytorch.org/docs/stable/nn.html#relu) activations. The input is a latent vector, $z$, which is derived from a standard normal distribution and the output is a 3x64x64 RGB image. The conv-transposed strided layers transform the latent vector into a volume having the same shape as an image.\n",
    "\n",
    "In this article, the authors also give some advice on how to configure optimizers, calculate loss functions and initialize model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "import random\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "\n",
    "Let's define some entries for the run :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = 'images/CFD_Images' # Root directory for the data set\n",
    "\n",
    "batch_size = 3**2  # Lot size during training\n",
    "image_size = 32 # Spatial size of training images.\n",
    "                #     All images will be resized as follows using a transformer.\n",
    "                #     This default implementation is 64x64.\n",
    "                #     If another size is desired, the structures of D and L must be modified.\n",
    "                #     See [here](https://github.com/pytorch/examples/issues/70)\n",
    "\n",
    "nc = 3   # Number of channels in the training images. (3 for color images)\n",
    "nz = 100 # Latent vector size z (i.e. generator input size)\n",
    "ngf = 32 # Size of entity cards in the generator\n",
    "ndf = 32 # Size of entity cards in the discriminator\n",
    "\n",
    "num_epochs = 5   # Number of training periods to be performed.\n",
    "                 # A longer training period will probably lead to better results,\n",
    "                 # but will also take much longer\n",
    "\n",
    "\n",
    "ngpu = 1        # Number of GPUs available. Use 0 for CPU mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "In this tutorial, we will use the [Chicago Face Database](https://chicagofaces.org/default/) dataset. The *CFD_Images* folder should be placed in the *images* folder:\n",
    "\n",
    "    ../images/CFD_Images\n",
    "       AF-200 -> AF-200\n",
    "           -> CFD-AF-200-228-N.jpg\n",
    "       FY-201 -> FY-201\n",
    "           -> CFD-AF-201-060-N.jpg\n",
    "          ...\n",
    "\n",
    "ImageFolder will be used to create the data set, it requires that there are subdirectories in the root folder (these subdirectories are used to create the data classes, which we will not be used here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use an image folder data set the way we configured it.\n",
    "\n",
    "# Transformation to be done on the training image\n",
    "import torchvision.transforms as transforms\n",
    "transform=transforms.Compose([transforms.Resize(image_size), # Resize the input PIL Image to the given size.\n",
    "                              transforms.CenterCrop(image_size), # Crops the given PIL Image at the center.\n",
    "                              transforms.ToTensor(), # Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\n",
    "                              transforms.Normalize(mean=(0.5, 0.5, 0.5), # Normalize a tensor image with mean and standard deviation.\n",
    "                                                   std=(0.5, 0.5, 0.5)) \n",
    "                             ]) \n",
    "\n",
    "\n",
    "\n",
    "# Create the data set\n",
    "import torchvision.datasets as dset\n",
    "dataset = dset.ImageFolder(root=dataroot, transform=transform)\n",
    "\n",
    "# Create the dataloader -- allows you to use the dataset with torch\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# Decide on which device we want to run on\n",
    "#device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display training images\n",
    "\n",
    "real_batch = next(iter(dataloader))\n",
    "# real_batch[0] -> images of a training batch\n",
    "# real_batch[1] -> classes of images of a training batch\n",
    "\n",
    "\n",
    "import torchvision.utils as vutils\n",
    "# we make a grid of images\n",
    "grid_images = vutils.make_grid(real_batch[0].to(device), padding=2, nrow=int(batch_size**.5), normalize=True).cpu()\n",
    "images = np.transpose(grid_images, (1,2,0)) # allows you to rearrange grid_images to display it\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(images);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight initialization\n",
    "\n",
    "\n",
    "According to the DCGAN article, the authors specify that all **the weights of the model must be randomly initialized** from a **normal distribution (mean=0 and stdev=0.02)**.\n",
    "\n",
    "The ``weights_init`` function takes an initialized model as input and resets all layers of convolutional, convolutional-transposition and batch normalization to meet this criterion. This function is applied to the models immediately after initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize customized weights called on the generator and discriminator networks\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "\n",
    "The generator, $G$, is designed to map the latent space vector ($z$) into the data space.\n",
    "\n",
    "Since our data is images, converting $z$ to data space means creating an RGB image of the same size as the training images (here 3x64x64). This is done by a series of transposed two-dimensional convolutional layers (**ConvTranspose2d**), each coupled to a layer of **BatchNorm2d** and an activation **ReLu**. The generator output is powered by a **Tanh** function to return to the input data range of $[-1.1]$. The functions of BatchNorm2d are located after the ConvTranspose2d layers, it is an essential contribution of the DCGAN document. These layers help the flow of gradients during training.\n",
    "\n",
    "An image of the generator from the DCGAN article is shown below.\n",
    "\n",
    "<img src=\"https://pytorch.org/tutorials/_images/dcgan_generator.png\" width=\"900\"/>\n",
    "\n",
    "The inputs defined in the input section (*nz*, *ngf*, and *nc*) influence the architecture of the generator :\n",
    "- **nz** is the length of the input vector z,\n",
    "- **ngf** refers to the size of the characteristic maps that are propagated by the generator,\n",
    "- **nc** is the number of channels in the output image (set to 3 for RGB images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[changing](https://github.com/pytorch/examples/issues/70)\n",
    "[changing](https://stackoverflow.com/questions/56957007/running-mean-error-in-python-pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Code\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            \n",
    "            # entry is Z, entering a convolution\n",
    "            \n",
    "            #nn.ConvTranspose2d(nz, ngf*8, 4, stride=1, padding=0, bias=False),\n",
    "            #nn.BatchNorm2d(ngf*8),\n",
    "            #nn.ReLU(True),\n",
    "            ## state size (ngf*8) x 4 x 4 x 4\n",
    "            \n",
    "            #nn.ConvTranspose2d(ngf*8, ngf*4, 4, stride=2, padding=1, bias=False),\n",
    "            nn.ConvTranspose2d(nz, ngf*4, 4, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(ngf*4),\n",
    "            nn.ReLU(True),\n",
    "            # state size (ngf*4) x 4 x 4 ##(ngf*4) x 8 x 8\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf*4, ngf*2, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ngf*2),\n",
    "            nn.ReLU(True),\n",
    "            # state size (ngf*2) x 8 x 8 ##(ngf*2) x 16 x 16\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf*2, ngf, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # size of states(ngf) x 16 x 16 ##(ngf) x 32 x 32\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf, nc, 4, stride=2, padding=1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size (nc) x 32 x 32 x 32 ##(nc) x 64 x 64\n",
    "        \n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "\n",
    "The $D$ discriminator is a binary classification network that takes an input image and emits a scalar probability that the input image is real (as opposed to false).\n",
    "\n",
    "Here, $D$ takes an input image 3x64x64, processes it through a series of layers **Conv2d**, **BatchNorm2d**, and **LeakyReLU**, and displays the final probability by a Sigmoid activation function.\n",
    "\n",
    "This architecture can be extended with other layers, but the use of stride convolution (here stride = 2 or 1), BatchNorm and LeakyReLU is important. The DCGAN document mentions that it is good practice to use striated convolution rather than pooling to reduce the sample because it allows the network to learn its own pooling function. In addition, the BatchNorm and LeakyReLU functions promote a balanced gradient flow, which is essential for the $G$ and $D$ learning process.\n",
    "\n",
    "<img src=\"https://sigmoidal.io/wp-content/uploads/2017/09/deep_convolutional_generative_adversarial_network1.png\" width=\"1000\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator Code\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            \n",
    "            # east input (nc) x 32 x 32 x 32 ## (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size (ndf) x 16 x 16 ## (ndf) x 32 x 32\n",
    "            \n",
    "            nn.Conv2d(ndf, ndf*2, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ndf*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size (ndf*2) x 8 x 8 ##(ndf*2) x 16 x 16\n",
    "            \n",
    "            nn.Conv2d(ndf*2, ndf*4, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ndf*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size (ndf*4) x 4 x 4 ##(ndf*4) x 8 x 8\n",
    "            \n",
    "            #nn.Conv2d(ndf*4, ndf*8, 4, stride=2, padding=1, bias=False),\n",
    "            #nn.BatchNorm2d(ndf*8),\n",
    "            #nn.LeakyReLU(0.2, inplace=True),\n",
    "            ## state size (ndf*8) x 4 x 4 x 4\n",
    "            \n",
    "            #nn.Conv2d(ndf*8, 1, 4, stride=1, padding=0, bias=False),\n",
    "            nn.Conv2d(ndf*4, 1, 4, stride=1, padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: adapted from\n",
    "[changing](https://github.com/pytorch/examples/issues/70)\n",
    "[changing](https://stackoverflow.com/questions/56957007/running-mean-error-in-python-pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can instantiate the generator and discriminator and apply the ``weights_init`` function. Check the returned model to see how the generator and discriminator objects are structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the generator\n",
    "netG = Generator(ngpu).to(device)\n",
    "\n",
    "# Create the discriminator\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "\n",
    "\n",
    "# Handle the multi-gpu if you wish\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights for mean=0, stdev=0.2.\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# Show models\n",
    "print(netG)\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions and optimizers\n",
    "\n",
    "With the $D$ and $G$ configuration, we can specify how they learn through loss functions and optimizers.\n",
    "\n",
    "- The binary entropy loss function [BCELoss](https://pytorch.org/docs/stable/nn.html#torch.nn.BCELoss) will be used. It is defined in PyTorch as :\n",
    "    \\begin{align}\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = - \\left[ y_n \\cdot \\log x_n + (1 - y_n) \\cdot \\log (1 - x_n) \\right]\\end{align}\n",
    "    This function provides the calculation of the two log components in the objective function ($log(D(x))$ and $log(1-D(G(z)))$).\n",
    "    \n",
    "    The part of the BCE equation to be used can be specified by simply changing $y$ (the labels):\n",
    "    - $l_n = -[\\log x_n]$ for $y=1$\n",
    "    - $l_n = - [\\log (1 - x_n)]$ for $y=0$\n",
    "\n",
    "- The actual label will be defined as 1 and the false label as 0 and will be used to calculate the losses of $D$ and $G$.\n",
    "\n",
    "- Two separate optimizers will be implemented, one for $D$ and the other for $G$. Both are Adam optimizers with a learning rate (lr) of 0.0002 and Beta1 = 0.5 (see DCGAN document).\n",
    "\n",
    "To follow the learning progress of the generator, we will generate a fixed batch of latent vectors that are derived from a Gaussian distribution (fixed_noise). In the learning loop, we will periodically enter this fixed_noise into $G$, and during iterations, we will see images formed from the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create a batch of latent vectors that we will use to visualize the progress of the generator\n",
    "fixed_noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish a convention for real and fake labels during training\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# Configure Adam optimizers for G and D\n",
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.0002     # Optimizer learning rate\n",
    "beta1 = 0.5     # Beta1 hyperparameter for Adam optimizers\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "\n",
    "GAN training is tricky, incorrect hyper-parameters cause the model to collapse without really knowing what went wrong.\n",
    "\n",
    "Here, we will follow algorithm 1 of Goodfellow's article, while respecting some of the best practices presented in [ganhacks](https://github.com/soumith/ganhacks). Namely, we will \"build different mini-lots for real and false images\", and also adjust the objective function of G to maximize $log(D(G(z)))$.\n",
    "\n",
    "The training is divided into two main parts. Part 1 updates the Discriminator and Part 2 updates the Generator.\n",
    "\n",
    "- **Part 1 - Discriminator Training**\n",
    "\n",
    "    The objective of this training is to maximize the probability of correctly classifying a given input as real or false. As far as Goodfellow is concerned, we want to \"update the discriminator by increasing its stochastic gradient\". In concrete terms, we want to **maximize $log(D(x)) + log(1-D(G(z)))$**. Due to the suggestion of separate mini-lots of ganhacks, we will calculate this in two steps:\n",
    "    1. Training with batches of **real images**:\n",
    "        1. Build a batch of real samples\n",
    "        2. Go through $D$\n",
    "        3. Calculate the loss ($log(D(x))$)\n",
    "        4. Calculate the gradients in a backward pass.\n",
    "    2. Training with batches of **fake images**:\n",
    "        1. Build a batch of false samples with the current generator\n",
    "        2. Pass this lot by $D$\n",
    "        3. Calculate the loss ($log(1-D(G(z)))$)\n",
    "        4. *Accumulate* the gradients with a back pass.\n",
    "    3. Update of the D parameters with an optimization step.\n",
    "\n",
    "Now, with the gradients accumulated from the all real and all false batches, we call a step of the Discriminator optimizer.\n",
    "\n",
    "- Part 2 - Generator Drive\n",
    "\n",
    "    We want to train the Generator by **minimizing $log(1-D(G(z)))$** to generate better forgeries. Goodfellow showed that this did not allow sufficient gradients to be obtained, especially at the beginning of the learning process. As a corrective measure, we would rather **maximize $log(D(G(z)))$**.\n",
    "\n",
    "    To classify the output of the Generator from Part 1 with the Discriminator, we will go through several steps:\n",
    "    1. Calculates the loss of G using **real labels** ($log(D(G(z)))$)\n",
    "    2. Calculates gradients of G in a backward pass\n",
    "    3. Update of G parameters with an optimization step.\n",
    "\n",
    "    Actual labels are used for the loss function, to use the $log(x)$ portion of the BCELoss (rather than the $log(1-x)$ portion)\n",
    "\n",
    "At the end of each era, we will push our batch of fixed noise (fixed_noise) through the generator to visually follow the progress of the G formation.\n",
    "\n",
    "The reported training statistics are:\n",
    "\n",
    "- **Loss_D** - loss of discriminator $log(D(x)) + log(1-D(G(z)))$ - sum of losses for all actual and fake batches.\n",
    "- **Loss_G** - loss of the generator $log(D(G(z)))$.\n",
    "- **D(x)** - average output of the discriminator for the actual batch. It should start at about 1 and converge to 0.5 as G improves.\n",
    "- **D(G(z))** - average output of the discriminator for the false batch. The first digit is before D is updated and the second digit is after D is updated. They should start near 0 and then converge to 0.5 as G improves.\n",
    "\n",
    "**Note :** This step may take some time, depending on the number of times you run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "# Lists to monitor the progress of projects\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "D_x_score = []\n",
    "D_G_z1_score = []\n",
    "D_G_z2_score = []\n",
    "\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "\n",
    "for epoch in range(num_epochs): # For each era\n",
    "    \n",
    "    for i, data in enumerate(dataloader, 0): # For each batch in the dataloader\n",
    "        \n",
    "        #====================================================================================================\n",
    "        \n",
    "        ###############################################################\n",
    "        # 1 - Network D update : maximize log(D(x)) + log(1 - D(G(z)) #\n",
    "        ###############################################################\n",
    "        \n",
    "        netD.zero_grad() # Sets the gradients of all model parameters to zero. (Backpropagation 1)\n",
    "        \n",
    "        #-----------------------------------------------------------------------\n",
    "        \n",
    "        # Training with batches of real images\n",
    "        #------------------------------------------\n",
    "        real_img = data[0].to(device) # x - Batch of real images\n",
    "        b_size = real_img.size(0) # number of images in the batch\n",
    "        \n",
    "        label = torch.full((b_size,), real_label, device=device) # real_label for each image of the real batch\n",
    "        output = netD(real_img).view(-1) # D(x) - Classifies the actual batch with D\n",
    "        errD_real = criterion(output, label) # log(D(x)) - calculates the loss of D on the actual batch\n",
    "        errD_real.backward()  # Calculates the gradients for D in steps backwards. (Backpropagation 2)\n",
    "        D_x = output.mean().item()\n",
    "        \n",
    "        # Training with batches of false images\n",
    "        #--------------------------------------------\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device) # z - Set of latent vectors\n",
    "        fake = netG(noise) # G(z) - Creates a batch of false images with G\n",
    "        \n",
    "        label.fill_(fake_label) # fake_label for each image of the fake batch\n",
    "        output = netD(fake.detach()).view(-1) # D(G(z)) - Classifies the wrong batch with D\n",
    "        errD_fake = criterion(output, label) # log(1-D(G(z))) - calculates the loss of D on the false batch\n",
    "        errD_fake.backward() # Calculates the gradients for D in steps backwards. (Backpropagation 2)\n",
    "        D_G_z1 = output.mean().item()\n",
    "        \n",
    "        #-----------------------------------------------------------------------\n",
    "        \n",
    "        errD = errD_real + errD_fake # log(D(x)) + log(1-D(G(z)) - sum of the gradients of the real and false batches.\n",
    "        optimizerD.step() # Update D (Backpropagation 3)\n",
    "        \n",
    "        ###############################################################\n",
    "        # 2 - Updating the G network: maximize log(D(G(z)))           #\n",
    "        ###############################################################\n",
    "        \n",
    "        netG.zero_grad() # Sets the gradients of all model parameters to zero. (Backpropagation 1)\n",
    "        \n",
    "        #-----------------------------------------------------------------------\n",
    "        \n",
    "        # Training with real labels  \n",
    "        #-----------------------------------\n",
    "        label.fill_(real_label) # real_label for each image of the false batch (for log(D(G(z)))\n",
    "        output = netD(fake).view(-1) # D(G(z)) - Classifies the wrong batch with D (with D already updated)\n",
    "        errG = criterion(output, label) # log(D(D(G(z))) - calculates the loss of G from this result.\n",
    "        errG.backward() # Calculates the gradients for G in steps backwards. (Backpropagation 2)\n",
    "        D_G_z2 = output.mean().item()\n",
    "        \n",
    "        #-----------------------------------------------------------------------\n",
    "        \n",
    "        optimizerG.step() # Update G (Backpropagation 3)\n",
    "        \n",
    "        #====================================================================================================\n",
    "        \n",
    "        \n",
    "        # Training statistics\n",
    "        if i % (len(dataloader)-1) == 0:\n",
    "            print('[%d/%d][%d/%d]    \\tLoss_D: %.3f\\tLoss_G: %.3f\\t\\tD(x): %.3f\\tD(G(z)): %.3f / %.3f'\n",
    "                  % (epoch+1, num_epochs, i+1, len(dataloader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "        \n",
    "        # Recording:\n",
    "        # - losses\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        \n",
    "        # - scores\n",
    "        D_x_score.append(D_x)\n",
    "        D_G_z1_score.append(D_G_z1)\n",
    "        D_G_z2_score.append(D_G_z2)\n",
    "        \n",
    "        \n",
    "        # Backup of G's output on fixed_noise.\n",
    "        if (iters % len(dataloader) == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, nrow=int(batch_size**.5), normalize=True))\n",
    "            \n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Loss versus training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score versus training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Score\")\n",
    "\n",
    "plt.plot(D_x_score,label=\"D(x)\")\n",
    "plt.plot(range(0,len(D_x_score), 50),\n",
    "         [np.mean(D_x_score[x:x+50]) for x in range(0,len(D_x_score), 50)], c='k')\n",
    "\n",
    "plt.plot(D_G_z1_score,label=\"D(G(z1))\")\n",
    "plt.plot(range(0,len(D_G_z1_score), 50),\n",
    "         [np.mean(D_G_z1_score[x:x+50]) for x in range(0,len(D_x_score), 50)], c='k')\n",
    "\n",
    "plt.plot(D_G_z2_score,label=\"D(G(z2))\")\n",
    "plt.plot(range(0,len(D_G_z2_score), 50),\n",
    "         [np.mean(D_G_z2_score[x:x+50]) for x in range(0,len(D_x_score), 50)], c='k')\n",
    "\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of G's progression\n",
    "\n",
    "Remember how we saved the generator output on the fixed noise batch after each training period. Now we can visualize the progress of G's training with an animation. Press the play button to start the animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i, (1,2,0)), animated=True)] for i in img_list]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Images vs. Fake Images\n",
    "\n",
    "Finally, let us look at real images and fake images side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Retrieve a batch of real images from the dataloader\n",
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "# Trace the real images\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device), nrow=int(batch_size**.5), padding=2, normalize=True).cpu(),(1,2,0)))\n",
    "#plt.show()\n",
    "\n",
    "# Trace the false images of the last epoch\n",
    "#plt.figure(figsize=(10,10))\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Fake Images\")\n",
    "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = time.time()\n",
    "print((fin-debut)//60, 'min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where to go next\n",
    "\n",
    "\n",
    "We have reached the end of our journey, but there are several places you can go from here. You could:\n",
    "\n",
    "- Train longer to see how good the results are\n",
    "- Modify this model to take a different data set and possibly modify the image size and architecture of the model.\n",
    "- Take a look at some other cool GAN projects [here](https://github.com/nashory/gans-awesome-applications)\n",
    "- Create GANs that generate [music](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
